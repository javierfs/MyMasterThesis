{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sanchezj/scipyenv/scipy2/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Sanchezj/scipyenv/scipy2/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import subprocess\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(__doc__)\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.cross_validation import  cross_val_score\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "filename = 'df_imputed_tot_OHE.pkl'\n",
    "df = pd.read_pickle(filename)\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "y = pd.Series(df['heartdisease'])\n",
    "\n",
    "\n",
    "train_X,test_X,train_y,test_y =train_test_split(X,y,test_size=0.33,shuffle = True, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    \"\"\"Report top n_top parameters settings, default n_top=3.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    grid_scores -- output from grid or random search\n",
    "    n_top -- how many to report, of top models\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] top parameter settings found in\n",
    "                  search\n",
    "    \"\"\"\n",
    "    top_scores = sorted(grid_scores,\n",
    "                        key=itemgetter(1),\n",
    "                        reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print((\"Mean validation score: \"\n",
    "               \"{0:.3f} (std: {1:.3f})\").format(\n",
    "               score.mean_validation_score,\n",
    "               np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "    return top_scores[0].parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_randomsearch(X, y, clf, para_dist, cv=5,\n",
    "                     n_iter_search=100):\n",
    "    \"\"\"Run a random search for best Decision Tree parameters.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    X -- features\n",
    "    y -- targets (classes)\n",
    "    cf -- scikit-learn Decision Tree\n",
    "    param_dist -- [dict] list, distributions of parameters\n",
    "                  to sample\n",
    "    cv -- fold of cross-validation, default 5\n",
    "    n_iter_search -- number of random parameter sets to try,\n",
    "                     default 20.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_params -- [dict] from report()\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(clf,\n",
    "                        param_distributions=param_dist,\n",
    "                        n_iter=n_iter_search)\n",
    "\n",
    "    start = time()\n",
    "    random_search.fit(X, y)\n",
    "    print((\"\\nRandomizedSearchCV took {:.2f} seconds \"\n",
    "           \"for {:d} candidates parameter \"\n",
    "           \"settings.\").format((time() - start),\n",
    "                               n_iter_search))\n",
    "\n",
    "    top_params = report(random_search.grid_scores_, 3)\n",
    "    return  top_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Random Parameter Search via 5-fold CV\n",
      "{'C': array([1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n",
      "       1.e+03, 1.e+04, 1.e+05]),\n",
      " 'penalty': ['l1', 'l2']}\n",
      "\n",
      "RandomizedSearchCV took 0.39 seconds for 22 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.788 (std: 0.021)\n",
      "Parameters: {'penalty': 'l2', 'C': 0.01}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.788 (std: 0.021)\n",
      "Parameters: {'penalty': 'l2', 'C': 0.1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.786 (std: 0.034)\n",
      "Parameters: {'penalty': 'l2', 'C': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Random Parameter Search via liblinear 10-fold CV\")\n",
    "#solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "\n",
    "    \n",
    "C_range = np.logspace(-5, 5, 11)\n",
    "penalty_range = ['l1', 'l2']\n",
    "\n",
    "param_dist = { 'penalty': penalty_range,\n",
    "               'C': C_range,}\n",
    "\n",
    "pprint(param_dist)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model_rs = run_randomsearch(X, y, lr_model, param_dist, cv=10, n_iter_search=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-- Testing best parameters [Random, CV = 10]...\n",
      "mean: 0.794 (std: 0.063)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the retuned best parameters\n",
    "print(\"\\n\\n-- Testing best parameters [Random, CV = 10]...\")\n",
    "lr_model_rs_final = LogisticRegression(**lr_model_rs)\n",
    "scores = cross_val_score(lr_model_rs_final, X, y, cv=10)\n",
    "print(\"mean: {:.3f} (std: {:.3f})\".format(scores.mean(),\n",
    "                                          scores.std()),\n",
    "                                          end=\"\\n\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for RF base is  0.8322368421052632\n",
      "Train Accuracy for RF base is  0.8084415584415584\n",
      "\n",
      "\n",
      "Test Accuracy for RF best is  0.8421052631578947\n",
      "Train Accuracy for RF best is  0.7808441558441559\n"
     ]
    }
   ],
   "source": [
    "lr_model_rs_best = LogisticRegression(**lr_model_rs)\n",
    "lr_model_rs_base = LogisticRegression() \n",
    "\n",
    "\n",
    "\n",
    "lr_model_rs_base.fit(train_X, train_y)\n",
    "lr_model_rs_best.fit(train_X, train_y)\n",
    "prediction_base=lr_model_rs_base.predict(test_X)\n",
    "prediction_best=lr_model_rs_best.predict(test_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prediction_base_train=lr_model_rs_base.predict(train_X)\n",
    "prediction_best_train=lr_model_rs_best.predict(train_X)\n",
    "\n",
    "print('Test Accuracy for RF base is ',metrics.accuracy_score(prediction_base,test_y))\n",
    "print('Train Accuracy for RF base is ',metrics.accuracy_score(train_y,prediction_base_train))\n",
    "print('\\n')\n",
    "print('Test Accuracy for RF best is ',metrics.accuracy_score(prediction_best,test_y))\n",
    "print('Train Accuracy for RF best is ',metrics.accuracy_score(train_y,prediction_best_train))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Random Parameter Search via liblinear 10-fold CV\n",
      "{'C': array([1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02,\n",
      "       1.e+03, 1.e+04, 1.e+05]),\n",
      " 'penalty': ['l2'],\n",
      " 'solver': ['newton-cg', 'lbfgs', 'sag']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sanchezj/scipyenv/scipy2/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/Sanchezj/scipyenv/scipy2/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RandomizedSearchCV took 0.89 seconds for 22 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.787 (std: 0.027)\n",
      "Parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.01}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.787 (std: 0.027)\n",
      "Parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.01}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.787 (std: 0.027)\n",
      "Parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.01}\n",
      "\n",
      "\n",
      "\n",
      "-- Testing best parameters [Random, CV = 10]...\n",
      "mean: 0.793 (std: 0.063)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "default: ‘liblinear’ Algorithm to use in the optimization problem.\n",
    "For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and\n",
    "‘saga’ are faster for large ones.\n",
    "For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\n",
    "handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas\n",
    "‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "'''\n",
    "\n",
    "print(\"-- Random Parameter Search via liblinear 10-fold CV\")\n",
    "#solver : {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},\n",
    "\n",
    "    \n",
    "C_range = np.logspace(-5, 5, 11)\n",
    "penalty_range = ['l2']\n",
    "solvers = ['newton-cg', 'lbfgs', 'sag']\n",
    "\n",
    "param_dist = { 'penalty': penalty_range,\n",
    "               'C': C_range,\n",
    "             'solver': solvers}\n",
    "\n",
    "pprint(param_dist)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model_rs = run_randomsearch(X, y, lr_model, param_dist, cv=10, n_iter_search=22)\n",
    "\n",
    "# test the retuned best parameters\n",
    "print(\"\\n\\n-- Testing best parameters [Random, CV = 10]...\")\n",
    "lr_model_rs_final = LogisticRegression(**lr_model_rs)\n",
    "scores = cross_val_score(lr_model_rs_final, X, y, cv=10)\n",
    "print(\"mean: {:.3f} (std: {:.3f})\".format(scores.mean(),\n",
    "                                          scores.std()),\n",
    "                                          end=\"\\n\\n\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
