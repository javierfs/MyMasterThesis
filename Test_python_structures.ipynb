{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from datetime import datetime as dt\n",
    "\n",
    "# Visualization packages:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = 'df_imputed_tot_OHE.pkl'\n",
    "df = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_0</th>\n",
       "      <th>age_1</th>\n",
       "      <th>age_2</th>\n",
       "      <th>age_3</th>\n",
       "      <th>age_4</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>sex_1</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>...</th>\n",
       "      <th>exang_0</th>\n",
       "      <th>exang_1</th>\n",
       "      <th>oldpeak_0</th>\n",
       "      <th>oldpeak_1</th>\n",
       "      <th>oldpeak_2</th>\n",
       "      <th>oldpeak_3</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "      <th>slope_3</th>\n",
       "      <th>heartdisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.213043</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>0.218478</td>\n",
       "      <td>0.191304</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>0.210870</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.189130</td>\n",
       "      <td>0.221739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633696</td>\n",
       "      <td>0.366304</td>\n",
       "      <td>0.415217</td>\n",
       "      <td>0.145652</td>\n",
       "      <td>0.142391</td>\n",
       "      <td>0.296739</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.068478</td>\n",
       "      <td>0.553261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.409681</td>\n",
       "      <td>0.406597</td>\n",
       "      <td>0.413438</td>\n",
       "      <td>0.393542</td>\n",
       "      <td>0.374494</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.408148</td>\n",
       "      <td>0.218063</td>\n",
       "      <td>0.391825</td>\n",
       "      <td>0.415642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482056</td>\n",
       "      <td>0.482056</td>\n",
       "      <td>0.493028</td>\n",
       "      <td>0.352949</td>\n",
       "      <td>0.349641</td>\n",
       "      <td>0.457069</td>\n",
       "      <td>0.497065</td>\n",
       "      <td>0.484386</td>\n",
       "      <td>0.252702</td>\n",
       "      <td>0.497426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age_0       age_1       age_2       age_3       age_4       sex_0  \\\n",
       "count  920.000000  920.000000  920.000000  920.000000  920.000000  920.000000   \n",
       "mean     0.213043    0.208696    0.218478    0.191304    0.168478    0.210870   \n",
       "std      0.409681    0.406597    0.413438    0.393542    0.374494    0.408148   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "            sex_1        cp_1        cp_2        cp_3      ...       \\\n",
       "count  920.000000  920.000000  920.000000  920.000000      ...        \n",
       "mean     0.789130    0.050000    0.189130    0.221739      ...        \n",
       "std      0.408148    0.218063    0.391825    0.415642      ...        \n",
       "min      0.000000    0.000000    0.000000    0.000000      ...        \n",
       "25%      1.000000    0.000000    0.000000    0.000000      ...        \n",
       "50%      1.000000    0.000000    0.000000    0.000000      ...        \n",
       "75%      1.000000    0.000000    0.000000    0.000000      ...        \n",
       "max      1.000000    1.000000    1.000000    1.000000      ...        \n",
       "\n",
       "          exang_0     exang_1   oldpeak_0   oldpeak_1   oldpeak_2   oldpeak_3  \\\n",
       "count  920.000000  920.000000  920.000000  920.000000  920.000000  920.000000   \n",
       "mean     0.633696    0.366304    0.415217    0.145652    0.142391    0.296739   \n",
       "std      0.482056    0.482056    0.493028    0.352949    0.349641    0.457069   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    0.000000    0.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          slope_1     slope_2     slope_3  heartdisease  \n",
       "count  920.000000  920.000000  920.000000    920.000000  \n",
       "mean     0.556522    0.375000    0.068478      0.553261  \n",
       "std      0.497065    0.484386    0.252702      0.497426  \n",
       "min      0.000000    0.000000    0.000000      0.000000  \n",
       "25%      0.000000    0.000000    0.000000      0.000000  \n",
       "50%      1.000000    0.000000    0.000000      1.000000  \n",
       "75%      1.000000    1.000000    0.000000      1.000000  \n",
       "max      1.000000    1.000000    1.000000      1.000000  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 40) (616,) (304, 40) (304,)\n"
     ]
    }
   ],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression\n",
    "from sklearn import svm #support vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.naive_bayes import GaussianNB #Naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n",
    "from sklearn.model_selection import train_test_split #training and testing data split\n",
    "from sklearn import metrics #accuracy measure\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "Y = pd.Series(df['heartdisease'])\n",
    "\n",
    "\n",
    "train_X,test_X,train_Y,test_Y =train_test_split(X,Y,test_size=0.33,shuffle = True, random_state=45)\n",
    "\n",
    "print(train_X.shape,train_Y.shape,test_X.shape, test_Y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for linear SVM is 0.8355263157894737\n",
      "F1 micro: is 0.8355263157894737\n",
      "F1 macro: is 0.831672203765227\n",
      "F1 weighted: is 0.8338506149093664\n"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='linear',C=0.1,gamma=0.001)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction2=model.predict(test_X)\n",
    "print('Accuracy for linear SVM is',metrics.accuracy_score(test_Y, prediction2))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction2, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction2, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction2, average='weighted'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for linear SVM is 0.8355263157894737\n",
      "F1 micro: is 0.8355263157894737\n",
      "F1 macro: is 0.831672203765227\n",
      "F1 weighted: is 0.8338506149093664\n"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction2=model.predict(test_X)\n",
    "print('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction2, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction2, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for rbf SVM is  0.8157894736842105\n",
      "F1 micro: is 0.8157894736842104\n",
      "F1 macro: is 0.8097877094972066\n",
      "F1 weighted: is 0.8126774478094676\n"
     ]
    }
   ],
   "source": [
    "model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction1=model.predict(test_X)\n",
    "print('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction1, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction1, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction1, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.8322368421052632\n",
      "F1 micro: is 0.8322368421052633\n",
      "F1 macro: is 0.8281286374671588\n",
      "F1 weighted: is 0.8304012613095144\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction3=model.predict(test_X)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction3, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction3, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction3, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e2fb021b1f76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "model=DecisionTreeClassifier()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction4=model.predict(test_X)\n",
    "print('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction4, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction4, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction4, average='weighted'))\n",
    "\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(model, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_index=list(range(1,101))\n",
    "a=pd.Series()\n",
    "x=list(range(1, 100))\n",
    "for i in list(range(1,101)):\n",
    "    model=KNeighborsClassifier(n_neighbors=i) \n",
    "    model.fit(train_X,train_Y)\n",
    "    prediction=model.predict(test_X)\n",
    "    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\n",
    "plt.plot(a_index, a)\n",
    "plt.xticks(x)\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(12,6)\n",
    "plt.show()\n",
    "print('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max(), ' with index: ', a.values.argmax(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GaussianNB()\n",
    "model.fit(train_X,train_Y)\n",
    "prediction6=model.predict(test_X)\n",
    "print('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction6, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction6, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction6, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_estimators=200)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction7=model.predict(test_X)\n",
    "print('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))\n",
    "print('F1 micro: is',f1_score(test_Y, prediction2, average='micro'))\n",
    "print('F1 macro: is',f1_score(test_Y, prediction2, average='macro'))\n",
    "print('F1 weighted: is',f1_score(test_Y, prediction2, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "xyz=[]\n",
    "accuracy=[]\n",
    "std=[]\n",
    "classifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\n",
    "models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model,X,Y, cv = kfold, scoring = \"accuracy\")\n",
    "    cv_result=cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    accuracy.append(cv_result)\n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \n",
    "new_models_dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "box=pd.DataFrame(accuracy,index=[classifiers])\n",
    "box.T.boxplot()\n",
    "plt.show()\n",
    "new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\n",
    "plt.title('Average CV Mean Accuracy')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(3,3,figsize=(12,10))\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\n",
    "ax[0,0].set_title('Matrix for rbf-SVM')\n",
    "y_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\n",
    "ax[0,1].set_title('Matrix for Linear-SVM')\n",
    "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\n",
    "ax[0,2].set_title('Matrix for KNN')\n",
    "y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\n",
    "ax[1,0].set_title('Matrix for Random-Forests')\n",
    "y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\n",
    "ax[1,1].set_title('Matrix for Logistic Regression')\n",
    "y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\n",
    "ax[1,2].set_title('Matrix for Decision Tree')\n",
    "y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\n",
    "ax[2,0].set_title('Matrix for Naive Bayes')\n",
    "plt.subplots_adjust(hspace=0.2,wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "C=[1e-3,1e-2,0.05,0.1]\n",
    "gamma=[1e-4,1e-3, 1e-2, 0.1]\n",
    "kernel=['rbf','linear']\n",
    "hyper={'kernel':kernel,'C':C,'gamma':gamma}\n",
    "gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "#\n",
    "# For an initial search, a logarithmic grid with basis\n",
    "# 10 is often helpful. Using a basis of 2, a finer\n",
    "# tuning can be achieved but at a much higher cost.\n",
    "from sklearn.model_selection import  StratifiedShuffleSplit, GridSearchCV\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "kernel_types=['rbf']\n",
    "param_grid = dict(kernel = kernel_types, gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
    "grid = GridSearchCV(estimator=svm.SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "\n",
    "# For an initial search, a logarithmic grid with basis\n",
    "# 10 is often helpful. Using a basis of 2, a finer\n",
    "# tuning can be achieved but at a much higher cost.\n",
    "from sklearn.model_selection import  StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "C_range = np.logspace(-5, 5, 11)\n",
    "gamma_range = np.logspace(-5, 5, 11)\n",
    "kernel_types=['rbf']\n",
    "param_grid = dict(kernel = kernel_types, gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.33, random_state=42)\n",
    "grid = GridSearchCV(estimator=svm.SVC(), param_grid=param_grid, cv=cv, verbose=True)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "\n",
    "# For an initial search, a logarithmic grid with basis\n",
    "# 10 is often helpful. Using a basis of 2, a finer\n",
    "# tuning can be achieved but at a much higher cost.\n",
    "from sklearn.model_selection import  StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "C_range = np.logspace(-5, 5, 11)\n",
    "gamma_range = np.logspace(-5, 5, 11)\n",
    "kernel_types=['linear']\n",
    "param_grid = dict(kernel = kernel_types, gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.33, random_state=42)\n",
    "grid = GridSearchCV(estimator=svm.SVC(), param_grid=param_grid, cv=cv, verbose=True)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_estimators=range(100,1000,100)\n",
    "hyper={'n_estimators':n_estimators}\n",
    "gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n",
    "                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.0001)),\n",
    "                                              ('RFor',RandomForestClassifier(n_estimators=400,random_state=0)),\n",
    "                                              ('LR',LogisticRegression(C=0.05)),\n",
    "                                              ('DT',DecisionTreeClassifier(random_state=0)),\n",
    "                                              ('NB',GaussianNB()),\n",
    "                                              ('svm',svm.SVC(kernel='linear',probability=True))\n",
    "                                             ], \n",
    "                       voting='soft').fit(train_X,train_Y)\n",
    "print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\n",
    "cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\n",
    "print('The cross validated score is',cross.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=79),random_state=0,n_estimators=700)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction=model.predict(test_X)\n",
    "print('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\n",
    "result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged KNN is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\n",
    "model.fit(train_X,train_Y)\n",
    "prediction=model.predict(test_X)\n",
    "print('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\n",
    "result=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for bagged Decision Tree is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier(n_estimators=400,random_state=0,learning_rate=0.01)\n",
    "result=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\n",
    "result=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Gradient Boosting is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "xgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\n",
    "result=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for XGBoost is:',result.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_estimators=list(range(100,1100,100))\n",
    "learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "hyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\n",
    "gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\n",
    "gd.fit(X,Y)\n",
    "print(gd.best_score_)\n",
    "print(gd.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\n",
    "result=cross_val_predict(ada,X,Y,cv=10)\n",
    "sns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(2,2,figsize=(15,12))\n",
    "model=RandomForestClassifier(n_estimators=500,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\n",
    "ax[0,0].set_title('Feature Importance in Random Forests')\n",
    "model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\n",
    "ax[0,1].set_title('Feature Importance in AdaBoost')\n",
    "model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\n",
    "ax[1,0].set_title('Feature Importance in Gradient Boosting')\n",
    "model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\n",
    "model.fit(X,Y)\n",
    "pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\n",
    "ax[1,1].set_title('Feature Importance in XgBoost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
